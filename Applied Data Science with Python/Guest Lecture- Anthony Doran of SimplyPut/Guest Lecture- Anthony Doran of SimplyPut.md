# Notes before we start

## UCSF Apex-enabled Research

Contact: Beth Berrean Beth.Berrean@ucsf.edu

Example projects with datasci work: [https://open-proposals.ucsf.edu/ai-ml-projects/2022](https://open-proposals.ucsf.edu/ai-ml-projects/2022)

Open job: [https://sjobs.brassring.com/TGnewUI/Search/home/HomeWithPreLoad?partnerid=6495&siteid=5861&PageType=JobDetails&jobid=3414138#jobDetails=3414138_5861](https://sjobs.brassring.com/TGnewUI/Search/home/HomeWithPreLoad?partnerid=6495&siteid=5861&PageType=JobDetails&jobid=3414138#jobDetails=3414138_5861)

# Guest speaker: Tony Doran

- Who is this guy?
- What is he talking about?
- How do I get to work with him?

![[counting_is_hard.webp]]

  

# Slides

![[Intro_to_Deep_Learning_Transformers.pdf]]

![[Intro_to_Deep_Learning__Transformers_(annotated).pdf]]

(note: automated annotations with some incorrect words/phrases, which is a decent example of the current state of the art in LLMs)

# # Links

## History

[https://synthedia.substack.com/p/the-history-of-large-language-models](https://synthedia.substack.com/p/the-history-of-large-language-models)

[https://www.youtube.com/watch?v=uocYQH0cWTs](https://www.youtube.com/watch?v=uocYQH0cWTs)

  

## Tokenization

[https://www.scaler.com/topics/nlp/subword-tokenization-algorithms/](https://www.scaler.com/topics/nlp/subword-tokenization-algorithms/)

[https://www.tensorflow.org/tutorials/text/word2vec](https://www.tensorflow.org/tutorials/text/word2vec)

word embeddings nailed in 2013

[https://arxiv.org/pdf/1310.4546.pdf](https://arxiv.org/pdf/1310.4546.pdf)

Sequence to Sequence (Encoder-Decoder)Tasks

[https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215) 2014

  

Attention Paper

[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)

Attention Viz

[https://erdem.pl/2021/05/introduction-to-attention-mechanism](https://erdem.pl/2021/05/introduction-to-attention-mechanism)

Transformers

“Attention is all you need” paper [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)

[https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)

Multi head attention

[https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853](https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853)

  

RLHF

[https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)

  

LLMS

List of open source LLMS

[https://github.com/eugeneyan/open-llms](https://github.com/eugeneyan/open-llms)

GPT (2018)

[https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)

Knowledge Distillation

[https://arxiv.org/pdf/1910.01108v4.pdf](https://arxiv.org/pdf/1910.01108v4.pdf)

  

  

Health CARE & AI

[https://thymia.ai/](https://thymia.ai/)su

[https://www.suki.ai/](https://www.suki.ai/)

[https://www.riken.jp/en/research/labs/bdr/](https://www.riken.jp/en/research/labs/bdr/)

[https://sites.research.google/med-palm/](https://sites.research.google/med-palm/)

  

Where to play around

[https://huggingface.co/learn/nlp-course/chapter3/2?fw=pt](https://huggingface.co/learn/nlp-course/chapter3/2?fw=pt)

[https://cloud.google.com/vertex-ai](https://cloud.google.com/vertex-ai)

[https://platform.openai.com/](https://platform.openai.com/)

[https://github.com/openai/evals](https://github.com/openai/evals)